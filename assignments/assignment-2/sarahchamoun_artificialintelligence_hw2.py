# -*- coding: utf-8 -*-
"""SarahChamoun_ArtificialIntelligence_HW2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mu7Sbyj_iC61X5IJjHclbydBEXaaAbUF
"""

!pip install ultralytics datasets pandas pyarrow

import os
from pathlib import Path

import pandas as pd
from datasets import load_dataset
from ultralytics import YOLO

# Base data folder
DATA_DIR = Path("assignment2_data")
FRAMES_DIR = DATA_DIR / "frames"

DATA_DIR.mkdir(exist_ok=True)
FRAMES_DIR.mkdir(parents=True, exist_ok=True)

# Video path (this is where we will save the YouTube video)
VIDEO_PATH = DATA_DIR / "input_video.mp4"

# Sampling: 1 frame every 5 seconds
FRAME_EVERY_N_SECONDS = 5

YOUTUBE_URL = "https://www.youtube.com/watch?v=YcvECxtXoxQ"

print("Saving video to:", VIDEO_PATH)

!yt-dlp -f "bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]" \
  -o "assignment2_data/input_video.mp4" \
  "https://www.youtube.com/watch?v=YcvECxtXoxQ"

!ls -lh assignment2_data

!ffmpeg -i assignment2_data/input_video.mp4

!ls -lh assignment2_data

# Clear old frames
!rm -f assignment2_data/frames/*.jpg

print("Extracting frames from:", VIDEO_PATH)

# 1 frame every 5 seconds → fps=1/5
!ffmpeg -y -nostdin -i assignment2_data/input_video.mp4 -vf "fps=1/5" assignment2_data/frames/frame_%04d.jpg

# Choose a small YOLO model
YOLO_WEIGHTS = "yolov8n.pt"  # ultralytics default tiny model
CONF_THRESHOLD = 0.25

model = YOLO(YOLO_WEIGHTS)
print("Model loaded. Class names:", model.names)

# List frames
frame_paths = sorted(FRAMES_DIR.glob("frame_*.jpg"))
print("Number of frames found:", len(frame_paths))
frame_paths[:5]

def run_detection_on_frames(
    frame_paths,
    model,
    video_id: str,
    frame_every_n_seconds: int,
    conf_threshold: float = 0.25,
) -> pd.DataFrame:
    rows = []

    for idx, frame_path in enumerate(frame_paths):
        frame_index = idx
        timestamp_sec = frame_index * frame_every_n_seconds

        results = model(str(frame_path), conf=conf_threshold)

        for r in results:  # one result per image
            boxes = r.boxes
            for box in boxes:
                cls_id = int(box.cls[0])
                conf = float(box.conf[0])

                # Bounding box in xyxy format
                x_min, y_min, x_max, y_max = box.xyxy[0].tolist()
                class_label = model.names[cls_id]

                rows.append(
                    {
                        "video_id": video_id,
                        "frame_index": frame_index,
                        "timestamp_sec": float(timestamp_sec),
                        "class_label": class_label,
                        "x_min": float(x_min),
                        "y_min": float(y_min),
                        "x_max": float(x_max),
                        "y_max": float(y_max),
                        "confidence_score": conf,
                    }
                )

    df = pd.DataFrame(rows)
    return df


VIDEO_ID = "rav4_review"
detections_df = run_detection_on_frames(
    frame_paths,
    model,
    video_id=VIDEO_ID,
    frame_every_n_seconds=FRAME_EVERY_N_SECONDS,
    conf_threshold=CONF_THRESHOLD,
)

detections_df.head()

PARQUET_PATH = DATA_DIR / "video_detections.parquet"
detections_df.to_parquet(PARQUET_PATH, index=False)
print("Saved detections to:", PARQUET_PATH)
print("Number of detections:", len(detections_df))

rav4_ds = load_dataset("aegean-ai/rav4-exterior-images", split="train")
rav4_ds

sample = rav4_ds[0]
print("Keys:", sample.keys())
print("Timestamp:", sample["timestamp"], "→", sample["timestamp_sec"])

from IPython.display import display
display(sample["image"])

def detect_main_label_on_image(model: YOLO, image, conf_threshold: float = 0.25):
    """
    Run YOLO on a single PIL image and return (label, confidence).
    If no detections, return (None, None).
    """
    results = model(image, conf=conf_threshold)
    r = results[0]

    if len(r.boxes) == 0:
        return None, None

    # pick the detection with highest confidence
    confidences = r.boxes.conf.tolist()
    best_idx = int(max(range(len(confidences)), key=lambda i: confidences[i]))

    box = r.boxes[best_idx]
    cls_id = int(box.cls[0])
    label = model.names[cls_id]
    conf = float(box.conf[0])

    return label, conf


# Test on one query
test_img = rav4_ds[0]["image"]
label, conf = detect_main_label_on_image(model, test_img, CONF_THRESHOLD)
print("Detected main label:", label, "with confidence:", conf)

detections = pd.read_parquet(PARQUET_PATH)
detections.head()

def retrieve_segments_for_query_image(
    image,
    detections_df: pd.DataFrame,
    model: YOLO,
    conf_threshold: float = 0.25,
    gap_threshold: float = 2.0,
):
    """
    Given a query image:
      1. Detect main label
      2. Filter detections in the video for that label
      3. Group into contiguous time segments
    """
    label, label_conf = detect_main_label_on_image(model, image, conf_threshold)

    if label is None:
        print("No detections found in query image.")
        return label, label_conf, []

    print(f"Main label for query image: {label} (conf={label_conf:.3f})")

    matches = detections_df[detections_df["class_label"] == label].copy()
    matches = matches.sort_values("timestamp_sec")

    if matches.empty:
        print("No matches found in video for this label.")
        return label, label_conf, []

    segments = []

    current_start = matches["timestamp_sec"].iloc[0]
    last_time = current_start
    count = 1

    for t in matches["timestamp_sec"].iloc[1:]:
        if t - last_time <= gap_threshold:
            # same segment
            last_time = t
            count += 1
        else:
            # segment ended
            segments.append(
                {
                    "start_timestamp": float(current_start),
                    "end_timestamp": float(last_time),
                    "class_label": label,
                    "number_of_supporting_detections": int(count),
                }
            )
            # start new segment
            current_start = t
            last_time = t
            count = 1

    # add last segment
    segments.append(
        {
            "start_timestamp": float(current_start),
            "end_timestamp": float(last_time),
            "class_label": label,
            "number_of_supporting_detections": int(count),
        }
    )

    return label, label_conf, segments

from IPython.display import display

def test_query(idx: int):
    print(f"\n=== Query index {idx} ===")
    sample = rav4_ds[idx]
    img = sample["image"]
    display(img)
    print("Dataset timestamp_sec:", sample["timestamp_sec"])

    label, label_conf, segments = retrieve_segments_for_query_image(
        img,
        detections,
        model,
        conf_threshold=CONF_THRESHOLD,
        gap_threshold=2.0,
    )

    print("\nRetrieved segments:")
    for seg in segments:
        print(seg)
        # Optional: YouTube link to verify
        start = int(seg["start_timestamp"])
        end = int(seg["end_timestamp"]) + 5  # add padding
        url = f"https://www.youtube.com/embed/YcvECxtXoxQ?start={start}&end={end}"
        print("YouTube check:", url)

# Try first 3 queries
for i in range(3):
    test_query(i)